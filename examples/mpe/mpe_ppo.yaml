seed: 0
lr: 7e-4
critic_lr: 7e-4
episode_length: 25
run_dir: ./run_results/
experiment_name: train_mpe
log_interval: 10
use_recurrent_policy: true
use_joint_action_loss: false
use_valuenorm: true
use_adv_normalize: true
wandb_entity: openrl-lab
data_chunk_length: 5
ppo_epoch: 5
# callbacks:
#   - id: "EvalCallback"
#     args: {
#       "eval_env": {"id": "simple_spread","env_num":10}, # how many envs to set up for evaluation
#       "n_eval_episodes": 100, # how many episodes to run for each evaluation
#       "eval_freq": 500, # how often to run evaluation
#       "log_path": "./results/eval_log_path", # where to save the evaluation results
#       "best_model_save_path": "./results/best_model/", # where to save the best model
#       "deterministic": True, # whether to use deterministic action
#       "render": False, # whether to render the env
#       "asynchronous": True, # whether to run evaluation asynchronously
#     }